{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c5bc69-e55c-4f34-9354-1a5fcd3ed349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict,load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PegasusTokenizerFast, DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BartTokenizer, BartModel\n",
    "from transformers import BartForConditionalGeneration\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7393df-323a-43c7-a4d1-6158c0f1b473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b11589-545a-43d1-a5c8-76de40a4ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_data(batch):\n",
    "    input_encodings = bart_base_tokenizer(batch['article'] , max_length = 1024, truncation = True )\n",
    "    with bart_base_tokenizer.as_target_tokenizer():\n",
    "        target_encodings = bart_base_tokenizer(batch['highlights'], max_length = 256, truncation = True )\n",
    "        \n",
    "    return_map = {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "    return return_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9df3ef-0344-48c8-9c44-cacdc65d22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_scores(dataset, metric, model, tokenizer, xlable, ylable, batch_size, device):\n",
    "    xlable_batches = []\n",
    "    ylable_batches = []\n",
    "    n = len(data)\n",
    "    for i in range(0, n, batch_size):\n",
    "        xlable_batch_data = dataset[xlable][i : i + batch_size]\n",
    "        ylable_batch_data = dataset[ylable][i : i + batch_size]\n",
    "        xlable_batches.append(xlable_batch_data)\n",
    "        ylable_batches.append(ylable_batch_data)\n",
    "\n",
    "    zipped_data = list(zip(xlable_batches, ylable_batches))\n",
    "    \n",
    "    for i in range(len(xlable_batches)):\n",
    "        x_label_batch = zipped_data[i][0]\n",
    "        y_label_batch = zipped_data[i][1]\n",
    "        \n",
    "        inputs = tokenizer(x_label_batch, max_length=1024,  truncation=True, \n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                         length_penalty=0.6, num_beams=6, max_length=256)\n",
    "        output_summaries = []\n",
    "        for summary in summaries:\n",
    "            output_summary = tokenizer.decode(summary, skip_special_tokens=True, \n",
    "                                clean_up_tokenization_spaces=True)\n",
    "            output_summaries.append(output_summary)  \n",
    "            \n",
    "        final_output_summaries = [output_summary.replace(\"\", \" \") for output_summary in output_summaries]\n",
    "        \n",
    "        metric.add_batch(predictions=final_output_summaries, references=y_label_batch)\n",
    "    \n",
    "    rouge_score = metric.compute()\n",
    "    return rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b387e3-1d93-48e8-bfd4-66227906e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetunellm(model,tokenizer,dataset, xlabel,ylabel,model_name,tokenizer_name,num_training,num_testing,num_validation,output_dir):\n",
    "    if(num_training != -1):\n",
    "        train_data = {xlabel: dataset[\"train\"][0:num_training][xlabel], ylabel: dataset[\"train\"][0:num_training][ylabel], \"id\": dataset[\"train\"][0:num_training]['id']}\n",
    "        test_data = {xlabel: dataset[\"test\"][0:num_testing][xlabel], ylabel: dataset[\"test\"][0:num_testing][ylabel], \"id\": dataset[\"test\"][0:num_testing]['id']}\n",
    "        validation_data = {xlabel: dataset[\"validation\"][0:num_validation][xlabel], ylabel: dataset[\"validation\"][0:num_validation][ylabel], \"id\": dataset[\"validation\"][0:num_validation]['id']}\n",
    "        train_dataset = Dataset.from_dict(train_data)\n",
    "        validation_dataset = Dataset.from_dict(validation_data)\n",
    "        test_dataset = Dataset.from_dict(test_data)\n",
    "        dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "    print(\"1.Dataset creation completed\")\n",
    "    encoded_dataset = dataset.map(encode_input_data, batched = True)\n",
    "    print(\"2.Dataset encoding completed\")\n",
    "    seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=output_dir, num_train_epochs=30, warmup_steps=500,per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,weight_decay=0.01, logging_steps=10,evaluation_strategy='steps',\n",
    "    eval_steps=500, save_steps=1000,gradient_accumulation_steps=16\n",
    "    ) \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"3.Starting Training of the Data\")\n",
    "    trainer = Trainer(model=model, args=training_args,\n",
    "              tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "              train_dataset=encoded_dataset[\"train\"], \n",
    "              eval_dataset=encoded_dataset[\"validation\"])\n",
    "    training_result = trainer.train()\n",
    "    print(\"4.Training completed\")\n",
    "    print(\"Training_result = \",training_result)\n",
    "    model.save_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(tokenizer_name)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d39dc-b0af-4f14-acd2-84b756e5964c",
   "metadata": {},
   "source": [
    "Model = Bart Base, Data Set = CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df618260-c6eb-496f-aa4f-f7c156591dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Dataset creation completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]/home/chandrakumar.s/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.24 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  6.77 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00,  7.39 examples/s]\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.Dataset encoding completed\n",
      "3.Starting Training of the Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:12, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.Training completed\n",
      "Training_result =  TrainOutput(global_step=30, training_loss=0.18704514106114706, metrics={'train_runtime': 16.5965, 'train_samples_per_second': 1.808, 'train_steps_per_second': 1.808, 'total_flos': 10092805632000.0, 'train_loss': 0.18704514106114706, 'epoch': 30.0})\n"
     ]
    }
   ],
   "source": [
    "bart_base_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base').to(device)\n",
    "bart_base_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "cnn_dataset = load_dataset(\"cnn_dailymail\",'3.0.0')\n",
    "xlable = 'article'\n",
    "ylabel = 'highlights'\n",
    "model_name = \"bart-cnn-model\"\n",
    "tokenizer_name = \"bart-cnn-tokenizer\"\n",
    "output_dir = \"Final-bart-cnn\"\n",
    "num_training = 10000\n",
    "num_testing = 1000\n",
    "num_validation = 1000\n",
    "dataset = finetunellm(bart_base_model,bart_base_tokenizer,cnn_dataset,xlable,ylabel,model_name,tokenizer_name,num_training,num_testing,num_validation,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f8c5bd-7446-4ead-9a2c-f2c7d0d8805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "input_text = dataset[\"train\"][0][xlable]\n",
    "output_summary = dataset_samsum_pt[\"train\"][0][ylabel]\n",
    "encoded_input = tokenizer(input_text, truncation=True, max_length=1024)\n",
    "decoded_summary = tokenizer.decode(encoded_input[\"input_ids\"], skip_special_tokens=True)\n",
    "gen_kwargs = {\"length_penalty\": 1, \"num_beams\":6, \"max_length\": 256}\n",
    "model_pipeline = pipeline(\"summarization\", model=model_name,tokenizer=tokenizer)\n",
    "print(\"\\nOutput Summary Predicted:\")\n",
    "print(model_pipeline(decoded_summary, **gen_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd05726-0961-4de8-a41c-65c9fb7729dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge')\n",
    "batch_size = 8\n",
    "score = get_rouge_scores(dataset['test'], rouge_metric, pretrained_model, tokenizer, xlable, ylabel, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f5005-8b44-4e9c-8000-887db556a4ad",
   "metadata": {},
   "source": [
    "Model = Pegasus-cnn_dailymail, Data Set = Government Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcc6844-8051-4d8b-8bfa-903e65232b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_cnn_dailymail_model = AutoModelForSeq2SeqLM.from_pretrained('google/pegasus-cnn_dailymail').to(device)\n",
    "pegasus_cnn_dailymail_tokenizer = PegasusTokenizerFast.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "government_dataset = load_dataset('ccdv/govreport-summarization')\n",
    "xlable = 'report'\n",
    "ylabel = 'summary'\n",
    "model_name = \"pegasus-government-model\"\n",
    "tokenizer_name = \"bart-cnn-tokenizer\"\n",
    "output_dir = \"Final-pegasus-government\"\n",
    "num_training = 1\n",
    "num_testing = 1\n",
    "num_validation = 1\n",
    "dataset = finetunellm(pegasus_cnn_dailymail_model,pegasus_cnn_dailymail_tokenizer,government_dataset,xlable,ylabel,model_name,tokenizer_name,num_training,num_testing,num_validation,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "462e4d92-acc6-4f51-a0d8-2008e008bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260213f2-5fd9-4a15-8965-fe5d2f4d35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge')\n",
    "batch_size = 8\n",
    "score = get_rouge_scores(dataset['test'], rouge_metric, pretrained_model, tokenizer, xlable, ylabel, batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f1823-d7db-4126-8275-606554d514b5",
   "metadata": {},
   "source": [
    "Model = Bart large CNN, Data Set = Samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09cf89d7-14b5-4382-a892-80f7b6034bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_largecnn_model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "bart_largecnn_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "samsum_dataset = load_dataset('samsum')\n",
    "xlable = 'report'\n",
    "ylabel = 'summary'\n",
    "model_name = \"bart-large-cnn-model\"\n",
    "tokenizer_name = \"bart-large-cnn-tokenizer\"\n",
    "output_dir = \"Final-bart-large-cnn-samsum\"\n",
    "num_training = -1\n",
    "num_testing = -1\n",
    "num_validation = -1\n",
    "dataset = finetunellm(bart_largecnn_model,bart_largecnn_tokenizer,samsum_dataset,xlable,ylabel,model_name,tokenizer_name,num_training,num_testing,num_validation,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77a0327c-256e-4bcc-9e0d-86127451e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05c492f7-ebd9-45f5-ae83-97b8450a1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge')\n",
    "batch_size = 8\n",
    "score = get_rouge_scores(dataset['test'], rouge_metric, pretrained_model, tokenizer, xlable, ylabel, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradingrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
